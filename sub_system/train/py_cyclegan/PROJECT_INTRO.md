# CycleGAN éŸ³è¨Šç‰¹å¾µåŸŸè½‰æ›ç³»çµ± | CycleGAN Audio Feature Domain Transfer System

[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/)
[![PyTorch Lightning](https://img.shields.io/badge/PyTorch%20Lightning-2.0+-purple.svg)](https://lightning.ai/)
[![Docker](https://img.shields.io/badge/Docker-Ready-green.svg)](https://www.docker.com/)
[![R](https://img.shields.io/badge/R-4.0+-276DC3.svg)](https://www.r-project.org/)

---

## ç›®éŒ„ | Table of Contents

- [å°ˆæ¡ˆç°¡ä»‹ | Project Overview](#å°ˆæ¡ˆç°¡ä»‹--project-overview)
- [ç³»çµ±æ¶æ§‹ | System Architecture](#ç³»çµ±æ¶æ§‹--system-architecture)
- [æŠ€è¡“æ£§ | Tech Stack](#æŠ€è¡“æ£§--tech-stack)
- [ç›®éŒ„çµæ§‹ | Directory Structure](#ç›®éŒ„çµæ§‹--directory-structure)
- [æ ¸å¿ƒåŠŸèƒ½ | Core Features](#æ ¸å¿ƒåŠŸèƒ½--core-features)
- [å®‰è£èˆ‡éƒ¨ç½² | Installation & Deployment](#å®‰è£èˆ‡éƒ¨ç½²--installation--deployment)
- [é…ç½®èªªæ˜ | Configuration](#é…ç½®èªªæ˜--configuration)
- [æ¨¡å‹è¨“ç·´æŒ‡å— | Model Training Guide](#æ¨¡å‹è¨“ç·´æŒ‡å—--model-training-guide)
- [API æ–‡æª” | API Documentation](#api-æ–‡æª”--api-documentation)
- [ä½¿ç”¨ç¯„ä¾‹ | Usage Examples](#ä½¿ç”¨ç¯„ä¾‹--usage-examples)
- [é–‹ç™¼æŒ‡å— | Development Guide](#é–‹ç™¼æŒ‡å—--development-guide)
- [æ•…éšœæ’é™¤ | Troubleshooting](#æ•…éšœæ’é™¤--troubleshooting)
- [ç¶­è­·èˆ‡å‡ç´š | Maintenance](#ç¶­è­·èˆ‡å‡ç´š--maintenance)

---

## å°ˆæ¡ˆç°¡ä»‹ | Project Overview

### ç¹é«”ä¸­æ–‡

æœ¬å°ˆæ¡ˆæ˜¯ä¸€å€‹åŸºæ–¼ **CycleGANï¼ˆå¾ªç’°ç”Ÿæˆå°æŠ—ç¶²è·¯ï¼‰** çš„éŸ³è¨Šç‰¹å¾µåŸŸè½‰æ›ç³»çµ±ï¼Œå°ˆæ³¨æ–¼å°‡ **CPC (Component - ä¸»æˆåˆ†åˆ†æ)** ç‰¹å¾µåŸŸè½‰æ›åˆ° **MA (Motor Analysis - é¦¬é”åˆ†æ)** ç‰¹å¾µåŸŸã€‚ç³»çµ±çµåˆäº†æ·±åº¦å­¸ç¿’ã€å‚³çµ±éŸ³è¨Šè™•ç†å’Œåˆ†æ•£å¼æ¶æ§‹ï¼Œæä¾›é«˜æ•ˆã€å¯æ“´å±•çš„éŸ³è¨Šåˆ†æè§£æ±ºæ–¹æ¡ˆã€‚

#### æ ¸å¿ƒåƒ¹å€¼

- **ç„¡ç›£ç£åŸŸè½‰æ›**ï¼šåˆ©ç”¨ CycleGAN å¯¦ç¾ç„¡éœ€é…å°æ•¸æ“šçš„ç‰¹å¾µåŸŸè½‰æ›
- **ç«¯åˆ°ç«¯è™•ç†**ï¼šå¾éŸ³è¨Šæª”æ¡ˆåˆ°ç‰¹å¾µæå–å†åˆ°åŸŸè½‰æ›çš„å®Œæ•´æµç¨‹
- **åˆ†æ•£å¼æ¶æ§‹**ï¼šåŸºæ–¼ RabbitMQ çš„éåŒæ­¥ä»»å‹™è™•ç†ï¼Œæ”¯æ´æ°´å¹³æ“´å±•
- **å®¹å™¨åŒ–éƒ¨ç½²**ï¼šå®Œæ•´çš„ Docker æ”¯æ´ï¼Œå¯å¿«é€Ÿéƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ
- **éˆæ´»é…ç½®**ï¼šè±å¯Œçš„é…ç½®é¸é …ï¼Œé©æ‡‰ä¸åŒçš„æ‡‰ç”¨å ´æ™¯

#### ä¸»è¦æ‡‰ç”¨å ´æ™¯

- é¦¬é”ç•°å¸¸æª¢æ¸¬çš„ç‰¹å¾µå·¥ç¨‹
- éŸ³è¨Šä¿¡è™Ÿçš„è·¨åŸŸåˆ†æ
- å·¥æ¥­è¨­å‚™å¥åº·ç›£æ¸¬
- è²å­¸ç‰¹å¾µå¢å¼·èˆ‡è½‰æ›

### English

This project is a **CycleGAN (Cycle-Consistent Generative Adversarial Network)** based audio feature domain transfer system, focusing on converting **CPC (Component - Principal Component Analysis)** features to **MA (Motor Analysis)** features. The system integrates deep learning, traditional audio processing, and distributed architecture to provide an efficient and scalable audio analysis solution.

#### Core Value

- **Unsupervised Domain Transfer**: Leverages CycleGAN for feature domain conversion without paired data
- **End-to-End Processing**: Complete pipeline from audio files to feature extraction to domain transfer
- **Distributed Architecture**: RabbitMQ-based asynchronous task processing with horizontal scalability
- **Containerized Deployment**: Full Docker support for rapid production deployment
- **Flexible Configuration**: Rich configuration options adaptable to various scenarios

#### Main Use Cases

- Feature engineering for motor anomaly detection
- Cross-domain audio signal analysis
- Industrial equipment health monitoring
- Acoustic feature enhancement and transformation

---

## ç³»çµ±æ¶æ§‹ | System Architecture

### æ¶æ§‹åœ– | Architecture Diagram

```mermaid
graph TB
    subgraph "å®¢æˆ¶ç«¯ Client"
        A[éŸ³è¨Šæª”æ¡ˆ<br/>Audio Files]
    end

    subgraph "API å±¤ API Layer"
        B[Flask REST API<br/>Health Check]
    end

    subgraph "è¨Šæ¯ä½‡åˆ— Message Queue"
        C[RabbitMQ<br/>Exchange: analyze.direct]
        D[Queue: analyze.step.sliced_wav...]
    end

    subgraph "è™•ç†æœå‹™ Processing Service"
        E[AudioProcessor<br/>éŸ³è¨Šè™•ç†å™¨]
        F[R Environment<br/>ç‰¹å¾µæå–]
        G[CycleGAN Model<br/>åŸŸè½‰æ›]
    end

    subgraph "æ•¸æ“šå­˜å„² Data Storage"
        H[(MongoDB)]
        I[(GridFS<br/>æª”æ¡ˆå­˜å„²)]
    end

    subgraph "æ¨¡å‹å±¤ Model Layer"
        J[Generator Aâ†’B<br/>ç”Ÿæˆå™¨]
        K[Generator Bâ†’A<br/>ç”Ÿæˆå™¨]
        L[Discriminator A<br/>åˆ¤åˆ¥å™¨]
        M[Discriminator B<br/>åˆ¤åˆ¥å™¨]
    end

    A -->|ä¸Šå‚³<br/>Upload| I
    B -->|å¥åº·æª¢æŸ¥<br/>Health Check| B
    C -->|æ¶ˆæ¯è·¯ç”±<br/>Route| D
    D -->|æ¶ˆè²»æ¶ˆæ¯<br/>Consume| E
    E -->|ä¸‹è¼‰æª”æ¡ˆ<br/>Download| I
    E -->|R è…³æœ¬åŸ·è¡Œ<br/>Execute R| F
    F -->|ç‰¹å¾µæ•¸æ“š<br/>Features| G
    G -->|è½‰æ›çµæœ<br/>Results| H
    E -->|æ›´æ–°ç‹€æ…‹<br/>Update Status| H
    G -.ä½¿ç”¨æ¨¡å‹<br/>Use Model.-> J
    G -.ä½¿ç”¨æ¨¡å‹<br/>Use Model.-> K
    J -.è¨“ç·´<br/>Train.-> L
    K -.è¨“ç·´<br/>Train.-> M

    style A fill:#e1f5ff
    style B fill:#fff4e1
    style G fill:#ffe1f5
    style H fill:#e1ffe1
    style I fill:#e1ffe1
```

### æ•¸æ“šæµç¨‹åœ– | Data Flow Diagram

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ¶ç«¯<br/>Client
    participant MQ as RabbitMQ
    participant API as Flask API
    participant Proc as AudioProcessor
    participant R as R Environment
    participant Model as CycleGAN
    participant DB as MongoDB

    Client->>DB: 1. ä¸Šå‚³éŸ³è¨Šæª”æ¡ˆåˆ° GridFS<br/>Upload audio to GridFS
    Client->>MQ: 2. ç™¼é€åˆ†æä»»å‹™æ¶ˆæ¯<br/>Send analysis task
    MQ->>Proc: 3. æ¶ˆè²»ä»»å‹™æ¶ˆæ¯<br/>Consume task
    Proc->>DB: 4. æŸ¥è©¢åˆ†æä»»å‹™<br/>Query analysis task
    Proc->>DB: 5. å¾ GridFS ä¸‹è¼‰æª”æ¡ˆ<br/>Download from GridFS
    Proc->>R: 6. è¨­ç½® R ç’°å¢ƒåƒæ•¸<br/>Setup R environment
    R->>R: 7. åŸ·è¡ŒéŸ³è¨Šç‰¹å¾µæå–<br/>Extract audio features
    R->>Proc: 8. è¿”å› CPC ç‰¹å¾µ<br/>Return CPC features
    Proc->>Model: 9. æ­£è¦åŒ– CPC ç‰¹å¾µ<br/>Normalize CPC features
    Model->>Model: 10. Generator Aâ†’B è½‰æ›<br/>Transform via Generator
    Model->>Proc: 11. è¿”å› MA ç‰¹å¾µ<br/>Return MA features
    Proc->>DB: 12. ä¿å­˜è½‰æ›çµæœ<br/>Save results
    Proc->>MQ: 13. ç™¼é€å®Œæˆé€šçŸ¥<br/>Send completion notification
    Client->>API: 14. æŸ¥è©¢è™•ç†ç‹€æ…‹<br/>Query status
    API->>DB: 15. è¿”å›åˆ†æçµæœ<br/>Return results
```

### çµ„ä»¶èªªæ˜ | Component Description

#### ç¹é«”ä¸­æ–‡

1. **Flask REST API**ï¼šæä¾›å¥åº·æª¢æŸ¥å’Œç³»çµ±ç‹€æ…‹æŸ¥è©¢ä»‹é¢
2. **RabbitMQ è¨Šæ¯ä½‡åˆ—**ï¼šå¯¦ç¾éåŒæ­¥ä»»å‹™è™•ç†å’Œæœå‹™è§£è€¦
3. **AudioProcessor**ï¼šæ ¸å¿ƒè™•ç†é‚è¼¯ï¼Œå”èª¿å„å€‹çµ„ä»¶å®Œæˆåˆ†æä»»å‹™
4. **R Environment**ï¼šä½¿ç”¨ rpy2 èª¿ç”¨ R è…³æœ¬é€²è¡ŒéŸ³è¨Šç‰¹å¾µæå–
5. **CycleGAN Model**ï¼šåŸºæ–¼ PyTorch Lightning çš„åŸŸè½‰æ›æ¨¡å‹
6. **MongoDB + GridFS**ï¼šå­˜å„²åˆ†æçµæœå’ŒéŸ³è¨Šæª”æ¡ˆ

#### English

1. **Flask REST API**: Provides health check and system status query endpoints
2. **RabbitMQ Message Queue**: Enables asynchronous task processing and service decoupling
3. **AudioProcessor**: Core processing logic coordinating components for analysis tasks
4. **R Environment**: Uses rpy2 to invoke R scripts for audio feature extraction
5. **CycleGAN Model**: PyTorch Lightning-based domain transfer model
6. **MongoDB + GridFS**: Stores analysis results and audio files

---

## æŠ€è¡“æ£§ | Tech Stack

### æ·±åº¦å­¸ç¿’æ¡†æ¶ | Deep Learning Framework

- **PyTorch**: æ·±åº¦å­¸ç¿’æ ¸å¿ƒæ¡†æ¶
- **PyTorch Lightning**: ç°¡åŒ–è¨“ç·´æµç¨‹å’Œå¯¦é©—ç®¡ç†
- **CycleGAN**: å¾ªç’°ä¸€è‡´æ€§ç”Ÿæˆå°æŠ—ç¶²è·¯

### å¾Œç«¯æœå‹™ | Backend Services

- **Flask**: REST API æ¡†æ¶
- **Python 3.10**: ä¸»è¦é–‹ç™¼èªè¨€
- **rpy2**: Python èˆ‡ R çš„æ©‹æ¥å™¨
- **R (Base + Libraries)**: éŸ³è¨Šç‰¹å¾µæå–

### æ•¸æ“šå­˜å„² | Data Storage

- **MongoDB**: NoSQL æ•¸æ“šåº«ï¼Œå­˜å„²åˆ†æçµæœ
- **GridFS**: MongoDB æª”æ¡ˆå­˜å„²ç³»çµ±ï¼Œè™•ç†å¤§å‹éŸ³è¨Šæª”æ¡ˆ

### è¨Šæ¯ä½‡åˆ— | Message Queue

- **RabbitMQ**: AMQP å”è­°è¨Šæ¯ä»£ç†ï¼Œå¯¦ç¾éåŒæ­¥ä»»å‹™è™•ç†
- **Pika**: Python çš„ RabbitMQ å®¢æˆ¶ç«¯

### éƒ¨ç½²èˆ‡å®¹å™¨åŒ– | Deployment & Containerization

- **Docker**: å®¹å™¨åŒ–å¹³å°
- **Ubuntu 22.04**: åŸºç¤æ˜ åƒ
- **PowerShell Scripts**: è‡ªå‹•åŒ–éƒ¨ç½²è…³æœ¬

### ä¸»è¦ Python å¥—ä»¶ | Key Python Packages

```
Flask==3.1.0
PyTorch Lightning
pymongo==4.10.1
pika==1.3.2
rpy2==3.5.14
numpy==2.0.2
pandas==2.2.3
```

---

## ç›®éŒ„çµæ§‹ | Directory Structure

```
py_cyclegan/
â”œâ”€â”€ cycleGan_model.py          # CycleGAN æ¨¡å‹å®šç¾© (Generator & Discriminator)
â”œâ”€â”€ pl_module.py               # PyTorch Lightning è¨“ç·´æ¨¡çµ„
â”œâ”€â”€ cpc_to_ma_converter.py     # CPC åˆ° MA åŸŸè½‰æ›å™¨
â”œâ”€â”€ customer_R_slice_Extract_Features.py  # Flask æœå‹™ä¸»ç¨‹å¼
â”œâ”€â”€ config.py                  # é…ç½®æª”æ¡ˆ
â”œâ”€â”€ output_LLM.py              # å°ˆæ¡ˆæ–‡ä»¶è¼¸å‡ºå·¥å…·
â”œâ”€â”€ test_v2.py                 # æ¸¬è©¦è…³æœ¬
â”œâ”€â”€ requirements.txt           # Python ä¾è³´
â”œâ”€â”€ Dockerfile                 # Docker æ˜ åƒå®šç¾©
â”œâ”€â”€ CD.md                      # éƒ¨ç½²æ–‡æª”
â”œâ”€â”€ PROJECT_INTRO.md           # æœ¬æ–‡æª”
â”‚
â”œâ”€â”€ saves/                     # æ¨¡å‹æª¢æŸ¥é»
â”‚   â””â”€â”€ Batchnorm_version.ckpt
â”‚
â”œâ”€â”€ test_model/                # æ¸¬è©¦æ¨¡å‹èˆ‡ç¯„ä¾‹
â”‚   â”œâ”€â”€ INPUT_FILE/
â”‚   â”‚   â””â”€â”€ dsf.json
â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â””â”€â”€ kkk.json
â”‚   â”œâ”€â”€ cpc_to_ma_converter.py
â”‚   â”œâ”€â”€ cycleGan_model.py
â”‚   â”œâ”€â”€ pl_module.py
â”‚   â””â”€â”€ saves/
â”‚       â””â”€â”€ Batchnorm_version.ckpt
â”‚
â””â”€â”€ logs/                      # æ—¥èªŒç›®éŒ„ (é‹è¡Œæ™‚å‰µå»º)
```

### æ ¸å¿ƒæª”æ¡ˆèªªæ˜ | Core File Description

#### `cycleGan_model.py`
å®šç¾© CycleGAN çš„ Generator å’Œ Discriminator æ¨¡å‹æ¶æ§‹ã€‚

**ç¹é«”ä¸­æ–‡**ï¼š
- `MotorGeneratorlinear`: ç·šæ€§è‡ªç·¨ç¢¼å™¨ç”Ÿæˆå™¨ï¼Œå°‡ç‰¹å¾µå¾ä¸€å€‹åŸŸæ˜ å°„åˆ°å¦ä¸€å€‹åŸŸ
- `MotorDiscriminatorLinear`: ç·šæ€§åˆ¤åˆ¥å™¨ï¼Œåˆ¤æ–·è¼¸å…¥ç‰¹å¾µçš„çœŸå½

**English**:
- `MotorGeneratorlinear`: Linear autoencoder generator for domain mapping
- `MotorDiscriminatorLinear`: Linear discriminator for authenticity classification

#### `pl_module.py`
PyTorch Lightning æ¨¡çµ„ï¼Œå¯¦ç¾å®Œæ•´çš„è¨“ç·´é‚è¼¯ã€‚

**Key Features | é—œéµç‰¹æ€§**ï¼š
- Cycle consistency loss (å¾ªç’°ä¸€è‡´æ€§æå¤±)
- Adversarial loss (å°æŠ—æå¤±)
- Manual optimization (æ‰‹å‹•å„ªåŒ–)
- TensorBoard logging (è¨“ç·´æ—¥èªŒè¨˜éŒ„)

#### `cpc_to_ma_converter.py`
ç¨ç«‹çš„è½‰æ›å™¨é¡åˆ¥ï¼Œç”¨æ–¼å°‡ CPC ç‰¹å¾µè½‰æ›ç‚º MA ç‰¹å¾µã€‚

**Pipeline | æµç¨‹**ï¼š
1. è¼‰å…¥æ¨¡å‹
2. è¼‰å…¥è¼¸å…¥æ•¸æ“š
3. æ­£è¦åŒ– CPC ç‰¹å¾µ
4. æ·»åŠ ä½ç½®ç·¨ç¢¼
5. åŸ·è¡ŒåŸŸè½‰æ›
6. åæ­£è¦åŒ– MA ç‰¹å¾µ
7. ä¿å­˜çµæœ

#### `customer_R_slice_Extract_Features.py`
Flask æœå‹™çš„ä¸»å…¥å£ï¼Œæ•´åˆæ‰€æœ‰çµ„ä»¶ã€‚

**Responsibilities | è·è²¬**ï¼š
- Flask API æœå‹™
- RabbitMQ æ¶ˆæ¯ç›£è½
- éŸ³è¨Šè™•ç†å”èª¿
- R ç’°å¢ƒç®¡ç†
- æ•¸æ“šåº«æ“ä½œ

#### `config.py`
é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®åƒæ•¸ï¼Œæ”¯æ´ç’°å¢ƒè®Šæ•¸è¦†è“‹ã€‚

---

## æ ¸å¿ƒåŠŸèƒ½ | Core Features

### 1. CycleGAN æ¨¡å‹æ¶æ§‹ | CycleGAN Model Architecture

#### ç”Ÿæˆå™¨ Generator

**ç¹é«”ä¸­æ–‡**ï¼š
ç”Ÿæˆå™¨æ¡ç”¨è‡ªç·¨ç¢¼å™¨æ¶æ§‹ï¼ŒåŒ…å«ç·¨ç¢¼å™¨å’Œè§£ç¢¼å™¨å…©éƒ¨åˆ†ï¼š

```
è¼¸å…¥ (10 ç¶­) â†’ Encoder â†’ æ½›åœ¨ç©ºé–“ (256 ç¶­) â†’ Decoder â†’ è¼¸å‡º (9 ç¶­)

Encoder:
- Linear(10, 64) + LeakyReLU + BatchNorm
- Linear(64, 128) + LeakyReLU + BatchNorm
- Linear(128, 256) + LeakyReLU + BatchNorm

Decoder:
- Linear(256, 128) + LeakyReLU + BatchNorm
- Linear(128, 64) + LeakyReLU + BatchNorm
- Linear(64, 9) + Tanh
```

**English**:
The generator uses an autoencoder architecture with encoder and decoder:

```
Input (10 dim) â†’ Encoder â†’ Latent Space (256 dim) â†’ Decoder â†’ Output (9 dim)
```

#### åˆ¤åˆ¥å™¨ Discriminator

**ç¹é«”ä¸­æ–‡**ï¼š
åˆ¤åˆ¥å™¨ä½¿ç”¨å¤šå±¤æ„ŸçŸ¥å™¨æ¶æ§‹ï¼Œè¼¸å‡ºçœŸå½æ©Ÿç‡ï¼š

```
è¼¸å…¥ (10 ç¶­) â†’ Linear(128) â†’ Linear(64) â†’ Linear(32) â†’ Linear(1) â†’ Sigmoid
```

#### è¨“ç·´ç­–ç•¥ Training Strategy

**Loss Functions | æå¤±å‡½æ•¸**ï¼š

1. **Adversarial Loss** (å°æŠ—æå¤±)ï¼š
   ```python
   BCE_loss = BCEWithLogitsLoss(D(x), real/fake)
   ```

2. **Cycle Consistency Loss** (å¾ªç’°ä¸€è‡´æ€§æå¤±)ï¼š
   ```python
   Cycle_loss = MSE(G_BA(G_AB(x)), x)
   ```

3. **Total Loss** (ç¸½æå¤±)ï¼š
   ```python
   Total = 0.1 * MSE_loss + 0.5 * Cycle_loss + 0.4 * Adversarial_loss
   ```

### 2. éŸ³è¨Šç‰¹å¾µæå– | Audio Feature Extraction

#### R ç’°å¢ƒæ•´åˆ | R Environment Integration

**ç¹é«”ä¸­æ–‡**ï¼š

ç³»çµ±ä½¿ç”¨ `rpy2` æ©‹æ¥ Python å’Œ Rï¼ŒåŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š

1. **ç’°å¢ƒè¨­ç½®**ï¼š
   - è¨­å®šå·¥ä½œç›®éŒ„
   - è¼‰å…¥ä¹‹å‰æ­¥é©Ÿçš„åˆ†ææ•¸æ“š
   - é…ç½® R åƒæ•¸

2. **ç‰¹å¾µæå–**ï¼š
   - åŸ·è¡Œ R è…³æœ¬ (`util_Analyze.R`, `util.R`, `3.3EF_lite.R`)
   - å¾éŸ³è¨Šç‰‡æ®µæå– 9 ç¶­ PC ç‰¹å¾µ

3. **æ•¸æ“šè½‰æ›**ï¼š
   - å°‡ R data.frame è½‰æ›ç‚º Python å­—å…¸åˆ—è¡¨
   - è™•ç†ä¸åŒæ•¸æ“šé¡å‹ (float, int, string)

**English**:

The system uses `rpy2` to bridge Python and R for:

1. **Environment Setup**: Configure working directories and R parameters
2. **Feature Extraction**: Execute R scripts for audio analysis
3. **Data Conversion**: Transform R data.frame to Python dict list

### 3. CPC åˆ° MA åŸŸè½‰æ› | CPC to MA Domain Transfer

#### è½‰æ›æµç¨‹ | Transfer Pipeline

```mermaid
graph LR
    A[CPC ç‰¹å¾µ<br/>CPC Features] -->|æ­£è¦åŒ–<br/>Normalize| B[æ¨™æº–åŒ–ç‰¹å¾µ<br/>Normalized]
    B -->|æ·»åŠ ä½ç½®ç·¨ç¢¼<br/>Add Position| C[å«ä½ç½®ç‰¹å¾µ<br/>With Position]
    C -->|Generator Aâ†’B<br/>Transfer| D[MA ç‰¹å¾µ æ­£è¦åŒ–<br/>MA Normalized]
    D -->|åæ­£è¦åŒ–<br/>Denormalize| E[MA ç‰¹å¾µ<br/>MA Features]

    style A fill:#e1f5ff
    style E fill:#ffe1f5
```

#### é—œéµæ­¥é©Ÿ | Key Steps

**1. Normalization | æ­£è¦åŒ–**

```python
cpc_mean = torch.mean(cpc_tensor, dim=0)
cpc_std = torch.std(cpc_tensor, dim=0)
cpc_normalized = (cpc_tensor - cpc_mean) / (cpc_std + 1e-5)
```

**2. Position Encoding | ä½ç½®ç·¨ç¢¼**

```python
position_encoding = torch.linspace(0, 1, len(cpc_normalized)).unsqueeze(1)
cpc_with_position = torch.cat([cpc_normalized, position_encoding], dim=1)
```

**3. Domain Transfer | åŸŸè½‰æ›**

```python
with torch.no_grad():
    ma_normalized = model.generator_A_to_B(cpc_with_position)
```

**4. Denormalization | åæ­£è¦åŒ–**

```python
ma_features = (ma_normalized * ma_std + ma_mean).cpu()
```

### 4. åˆ†æ•£å¼ä»»å‹™è™•ç† | Distributed Task Processing

#### RabbitMQ è¨Šæ¯æµ | RabbitMQ Message Flow

**ç¹é«”ä¸­æ–‡**ï¼š

1. **ä»»å‹™æäº¤**ï¼š
   - å®¢æˆ¶ç«¯ä¸Šå‚³éŸ³è¨Šæª”æ¡ˆåˆ° GridFS
   - ç™¼é€ä»»å‹™æ¶ˆæ¯åˆ° RabbitMQ exchange

2. **ä»»å‹™åˆ†ç™¼**ï¼š
   - Exchange æ ¹æ“š routing key è·¯ç”±æ¶ˆæ¯
   - ä»»å‹™ä½‡åˆ—æ¥æ”¶æ¶ˆæ¯

3. **ä»»å‹™è™•ç†**ï¼š
   - Worker å¾ä½‡åˆ—æ¶ˆè²»æ¶ˆæ¯
   - æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚º "processing"
   - åŸ·è¡ŒéŸ³è¨Šè™•ç†æµç¨‹

4. **çµæœé€šçŸ¥**ï¼š
   - è™•ç†å®Œæˆå¾Œæ›´æ–°ç‹€æ…‹ç‚º "completed"
   - ç™¼é€å®Œæˆé€šçŸ¥åˆ° state check exchange

**English**:

1. **Task Submission**: Upload audio to GridFS, send task message
2. **Task Distribution**: Exchange routes messages by routing key
3. **Task Processing**: Worker consumes and processes tasks
4. **Result Notification**: Update status and send completion notification

---

## å®‰è£èˆ‡éƒ¨ç½² | Installation & Deployment

### ç’°å¢ƒè¦æ±‚ | Prerequisites

**ç¹é«”ä¸­æ–‡**ï¼š

- **ä½œæ¥­ç³»çµ±**ï¼šUbuntu 22.04 æˆ– Windows (with Docker)
- **Python**ï¼š3.10+
- **R**ï¼š4.0+
- **Docker**ï¼š20.10+
- **MongoDB**ï¼š4.4+
- **RabbitMQ**ï¼š3.8+

**English**:

- **OS**: Ubuntu 22.04 or Windows (with Docker)
- **Python**: 3.10+
- **R**: 4.0+
- **Docker**: 20.10+
- **MongoDB**: 4.4+
- **RabbitMQ**: 3.8+

### æœ¬åœ°é–‹ç™¼ç’°å¢ƒè¨­ç½® | Local Development Setup

#### 1. å®‰è£ Python ä¾è³´ | Install Python Dependencies

```bash
# å‰µå»ºè™›æ“¬ç’°å¢ƒ | Create virtual environment
python -m venv .venv

# å•Ÿå‹•è™›æ“¬ç’°å¢ƒ | Activate virtual environment
# Windows:
.venv\Scripts\activate
# Linux/Mac:
source .venv/bin/activate

# å®‰è£ä¾è³´ | Install dependencies
pip install -r requirements.txt
pip install rpy2==3.5.14
```

#### 2. å®‰è£ R å’Œç›¸é—œå¥—ä»¶ | Install R and Packages

```bash
# Ubuntu
sudo apt-get update
sudo apt-get install r-base r-base-dev

# åŸ·è¡Œ R å¥—ä»¶å®‰è£è…³æœ¬
Rscript install_r_packages.R
```

#### 3. é…ç½®ç’°å¢ƒè®Šæ•¸ | Configure Environment Variables

å‰µå»º `.env` æª”æ¡ˆ | Create `.env` file:

```bash
# MongoDB é…ç½®
MONGO_URI=mongodb://user:password@host:port
MONGO_DB=sound_analysis

# RabbitMQ é…ç½®
RABBITMQ_HOST=localhost
RABBITMQ_PORT=5672
RABBITMQ_USER=guest
RABBITMQ_PASS=guest

# Flask é…ç½®
FLASK_HOST=0.0.0.0
FLASK_PORT=57122
DEBUG=False

# æœå‹™é…ç½®
SERVER_NAME=customer_R_slice_Extract_Features
SERVER_VISION=1.0.0
THE_STEP=4
```

#### 4. é‹è¡Œæœå‹™ | Run Service

```bash
python customer_R_slice_Extract_Features.py
```

### Docker éƒ¨ç½² | Docker Deployment

#### æ§‹å»º Docker æ˜ åƒ | Build Docker Image

**ä½¿ç”¨éƒ¨ç½²è…³æœ¬ | Using Deployment Script**:

```powershell
# Windows PowerShell
.\scripts\build.ps1 1.0.6
```

**æ‰‹å‹•æ§‹å»º | Manual Build**:

```bash
docker build -t your-registry/py_cyclegan:1.0.6 .
```

#### é‹è¡Œå®¹å™¨ | Run Container

**ä½¿ç”¨éƒ¨ç½²è…³æœ¬ | Using Deployment Script**:

```powershell
.\scripts\deploy.ps1 1.0.6
```

**æ‰‹å‹•é‹è¡Œ | Manual Run**:

```bash
docker run -d \
  --name py_cyclegan \
  -p 57122:57122 \
  -e MONGO_URI=mongodb://user:password@host:port \
  -e RABBITMQ_HOST=rabbitmq-host \
  your-registry/py_cyclegan:1.0.6
```

### é›†ç¾¤éƒ¨ç½² | Cluster Deployment

#### éƒ¨ç½²åˆ° Docker Swarm æˆ– Kubernetes

**ä½¿ç”¨é›†ç¾¤éƒ¨ç½²è…³æœ¬ | Using Cluster Script**:

```powershell
# éƒ¨ç½² | Deploy
.\scripts\docker_sound_analysis_cluster_deploy.ps1 1.0.6

# åˆªé™¤ | Remove
.\scripts\docker_sound_analysis_cluster_delete.ps1
```

### éƒ¨ç½²è…³æœ¬è©³ç´°èªªæ˜ | Deployment Scripts Details

#### `build.ps1` - æ§‹å»ºè…³æœ¬

**ç¹é«”ä¸­æ–‡**ï¼š

æ­¤è…³æœ¬è² è²¬æ§‹å»º Docker æ˜ åƒä¸¦æ¨é€åˆ° Registryã€‚

**åƒæ•¸**ï¼š
- `$VERSION`: Docker æ˜ åƒç‰ˆæœ¬è™Ÿ (ä¾‹å¦‚ï¼š1.0.6)

**åŸ·è¡Œå…§å®¹**ï¼š
1. é©—è­‰ Dockerfile å­˜åœ¨
2. æ§‹å»º Docker æ˜ åƒ
3. ç‚ºæ˜ åƒæ‰“ä¸Šç‰ˆæœ¬æ¨™ç±¤å’Œ latest æ¨™ç±¤
4. æ¨é€æ˜ åƒåˆ° Docker Registry

**ä½¿ç”¨ç¯„ä¾‹**ï¼š
```powershell
.\scripts\build.ps1 1.0.6
```

**English**:

This script builds Docker image and pushes to registry.

**Parameters**:
- `$VERSION`: Docker image version (e.g., 1.0.6)

**Actions**:
1. Verify Dockerfile exists
2. Build Docker image
3. Tag with version and latest
4. Push to Docker Registry

#### `deploy.ps1` - å–®æ©Ÿéƒ¨ç½²è…³æœ¬

**ç¹é«”ä¸­æ–‡**ï¼š

æ­¤è…³æœ¬ç”¨æ–¼åœ¨å–®å°æ©Ÿå™¨ä¸Šéƒ¨ç½²æˆ–æ›´æ–°å®¹å™¨ã€‚

**åƒæ•¸**ï¼š
- `$VERSION`: Docker æ˜ åƒç‰ˆæœ¬è™Ÿ (å¯é¸ï¼Œé è¨­ç‚º latest)

**åŸ·è¡Œå…§å®¹**ï¼š
1. åœæ­¢ä¸¦åˆªé™¤ç¾æœ‰å®¹å™¨
2. æ‹‰å–æŒ‡å®šç‰ˆæœ¬çš„æ˜ åƒ
3. å‰µå»ºä¸¦å•Ÿå‹•æ–°å®¹å™¨
4. é…ç½®ç’°å¢ƒè®Šæ•¸å’Œç«¯å£æ˜ å°„

**ä½¿ç”¨ç¯„ä¾‹**ï¼š
```powershell
# ä½¿ç”¨æŒ‡å®šç‰ˆæœ¬
.\scripts\deploy.ps1 1.0.6

# ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
.\scripts\deploy.ps1
```

**English**:

Deploys or updates container on a single machine.

**Parameters**:
- `$VERSION`: Docker image version (optional, defaults to latest)

**Actions**:
1. Stop and remove existing container
2. Pull specified image version
3. Create and start new container
4. Configure environment variables and port mappings

#### `docker_sound_analysis_cluster_deploy.ps1` - é›†ç¾¤éƒ¨ç½²è…³æœ¬

**ç¹é«”ä¸­æ–‡**ï¼š

æ­¤è…³æœ¬ç”¨æ–¼éƒ¨ç½²æœå‹™åˆ° Docker Swarm é›†ç¾¤ã€‚

**åƒæ•¸**ï¼š
- `$VERSION`: Docker æ˜ åƒç‰ˆæœ¬è™Ÿ

**åŸ·è¡Œå…§å®¹**ï¼š
1. åˆå§‹åŒ– Docker Swarm (å¦‚æœå°šæœªåˆå§‹åŒ–)
2. å‰µå»º overlay ç¶²è·¯
3. éƒ¨ç½²æœå‹™ stack
4. é…ç½®æœå‹™å‰¯æœ¬æ•¸å’Œæ›´æ–°ç­–ç•¥
5. è¨­ç½®å¥åº·æª¢æŸ¥

**é…ç½®é¸é …**ï¼š
```powershell
$REPLICAS = 3  # æœå‹™å‰¯æœ¬æ•¸
$NETWORK_NAME = "sound_analysis_network"
```

**ä½¿ç”¨ç¯„ä¾‹**ï¼š
```powershell
.\scripts\docker_sound_analysis_cluster_deploy.ps1 1.0.6
```

**English**:

Deploys service to Docker Swarm cluster.

**Parameters**:
- `$VERSION`: Docker image version

**Actions**:
1. Initialize Docker Swarm if needed
2. Create overlay network
3. Deploy service stack
4. Configure replicas and update strategy
5. Setup health checks

#### `docker_sound_analysis_cluster_delete.ps1` - é›†ç¾¤åˆªé™¤è…³æœ¬

**ç¹é«”ä¸­æ–‡**ï¼š

æ­¤è…³æœ¬ç”¨æ–¼å¾é›†ç¾¤ä¸­åˆªé™¤æœå‹™ã€‚

**åŸ·è¡Œå…§å®¹**ï¼š
1. åœæ­¢æ‰€æœ‰æœå‹™å‰¯æœ¬
2. åˆªé™¤æœå‹™å®šç¾©
3. æ¸…ç†ç›¸é—œè³‡æº

**ä½¿ç”¨ç¯„ä¾‹**ï¼š
```powershell
.\scripts\docker_sound_analysis_cluster_delete.ps1
```

**English**:

Removes service from cluster.

**Actions**:
1. Stop all service replicas
2. Remove service definition
3. Clean up resources

---

## é…ç½®èªªæ˜ | Configuration

### `config.py` åƒæ•¸è©³è§£ | Parameter Details

#### MongoDB é…ç½® | MongoDB Configuration

```python
# é€£æ¥ URI | Connection URI
MONGO_URI = os.getenv("MONGO_URI", "mongodb://user:password@host:port")

# æ•¸æ“šåº«åç¨± | Database name
MONGO_DB = os.getenv("MONGO_DB", "sound_analysis")
```

**ç¹é«”ä¸­æ–‡**ï¼š
- `MONGO_URI`: MongoDB é€£æ¥å­—ä¸²ï¼ŒåŒ…å«èªè­‰è³‡è¨Š
- `MONGO_DB`: ä½¿ç”¨çš„æ•¸æ“šåº«åç¨±

**English**:
- `MONGO_URI`: MongoDB connection string with authentication
- `MONGO_DB`: Database name to use

#### RabbitMQ é…ç½® | RabbitMQ Configuration

```python
RABBITMQ_HOST = os.getenv("RABBITMQ_HOST", "localhost")
RABBITMQ_PORT = int(os.getenv("RABBITMQ_PORT", "5672"))
RABBITMQ_USER = os.getenv("RABBITMQ_USER", "guest")
RABBITMQ_PASS = os.getenv("RABBITMQ_PASS", "guest")
RABBITMQ_VHOST = os.getenv("RABBITMQ_VHOST", "/")

EXCHANGE_NAME = os.getenv("EXCHANGE_NAME", "analyze.direct")
QUEUE_NAME = os.getenv("QUEUE_NAME", "analyze.step.sliced_wav_to_4col_AE_Features_324a")
ROUTING_KEY = os.getenv("ROUTING_KEY", "analyze.step.sliced_wav_to_4col_AE_Features_324a")
```

**ç¹é«”ä¸­æ–‡**ï¼š
- `EXCHANGE_NAME`: äº¤æ›æ©Ÿåç¨±ï¼Œé¡å‹ç‚º direct
- `QUEUE_NAME`: ä½‡åˆ—åç¨±
- `ROUTING_KEY`: è·¯ç”±éµï¼Œç”¨æ–¼æ¶ˆæ¯è·¯ç”±

**English**:
- `EXCHANGE_NAME`: Exchange name, type is direct
- `QUEUE_NAME`: Queue name
- `ROUTING_KEY`: Routing key for message routing

#### Flask é…ç½® | Flask Configuration

```python
FLASK_HOST = os.getenv("FLASK_HOST", "0.0.0.0")
FLASK_PORT = int(os.getenv("FLASK_PORT", "57122"))
DEBUG = os.getenv("DEBUG", "False").lower() == "true"
```

**ç¹é«”ä¸­æ–‡**ï¼š
- `FLASK_HOST`: æœå‹™ç›£è½åœ°å€ï¼Œ0.0.0.0 è¡¨ç¤ºç›£è½æ‰€æœ‰ä»‹é¢
- `FLASK_PORT`: æœå‹™ç«¯å£
- `DEBUG`: æ˜¯å¦å•Ÿç”¨èª¿è©¦æ¨¡å¼ (ç”Ÿç”¢ç’°å¢ƒæ‡‰è¨­ç‚º False)

**English**:
- `FLASK_HOST`: Service listening address, 0.0.0.0 for all interfaces
- `FLASK_PORT`: Service port
- `DEBUG`: Enable debug mode (should be False in production)

#### è™•ç†æ­¥é©Ÿè¨­å®š | Processing Step Configuration

```python
THE_STEP = int(os.getenv("THE_STEP", "4"))
```

**ç¹é«”ä¸­æ–‡**ï¼š
æ­¤æœå‹™åœ¨åˆ†ææµç¨‹ä¸­çš„æ­¥é©Ÿç·¨è™Ÿï¼Œç”¨æ–¼åœ¨ MongoDB ä¸­æ¨™è­˜è™•ç†éšæ®µã€‚

**English**:
Step number in the analysis pipeline, used to identify processing stage in MongoDB.

#### åˆ†æè¨­å®š | Analysis Configuration

```python
ANALYZE_CHUNK_SIZE = int(os.getenv("ANALYZE_CHUNK_SIZE", "1000"))
ANALYZE_MAX_RETRIES = int(os.getenv("ANALYZE_MAX_RETRIES", "3"))
ANALYZE_TIMEOUT = int(os.getenv("ANALYZE_TIMEOUT", "3600"))
```

**ç¹é«”ä¸­æ–‡**ï¼š
- `ANALYZE_CHUNK_SIZE`: è™•ç†å¤§é‡æ•¸æ“šæ™‚çš„æ‰¹æ¬¡å¤§å°
- `ANALYZE_MAX_RETRIES`: è™•ç†å¤±æ•—æ™‚çš„é‡è©¦æ¬¡æ•¸
- `ANALYZE_TIMEOUT`: åˆ†æè¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰

**English**:
- `ANALYZE_CHUNK_SIZE`: Batch size for processing large datasets
- `ANALYZE_MAX_RETRIES`: Number of retries on failure
- `ANALYZE_TIMEOUT`: Analysis timeout in seconds

#### æª”æ¡ˆè™•ç†è¨­å®š | File Processing Configuration

```python
FILE_UPLOAD_MAX_SIZE = int(os.getenv("FILE_UPLOAD_MAX_SIZE", "104857600"))  # 100MB
ALLOWED_EXTENSIONS = os.getenv("ALLOWED_EXTENSIONS", "wav").split(",")
```

**ç¹é«”ä¸­æ–‡**ï¼š
- `FILE_UPLOAD_MAX_SIZE`: é™åˆ¶ä¸Šå‚³æª”æ¡ˆå¤§å°ï¼ˆä½å…ƒçµ„ï¼‰
- `ALLOWED_EXTENSIONS`: å…è¨±çš„æª”æ¡ˆé¡å‹

**English**:
- `FILE_UPLOAD_MAX_SIZE`: Upload file size limit in bytes
- `ALLOWED_EXTENSIONS`: Allowed file types

#### è·¯å¾‘è¨­å®š | Path Configuration

```python
PYTHON_PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
TEMP_DIR = os.getenv("TEMP_DIR", os.path.join(PYTHON_PROJECT_ROOT, "temp"))
```

**ç¹é«”ä¸­æ–‡**ï¼š
- `PYTHON_PROJECT_ROOT`: å°ˆæ¡ˆæ ¹ç›®éŒ„
- `TEMP_DIR`: è‡¨æ™‚æª”æ¡ˆå­˜æ”¾ç›®éŒ„

**English**:
- `PYTHON_PROJECT_ROOT`: Project root directory
- `TEMP_DIR`: Temporary files directory

#### æ•ˆèƒ½å„ªåŒ–è¨­å®š | Performance Configuration

```python
WORKERS = int(os.getenv("WORKERS", "4"))
THREADS = int(os.getenv("THREADS", "2"))
```

**ç¹é«”ä¸­æ–‡**ï¼š
- `WORKERS`: è™•ç†é€²ç¨‹æ•¸
- `THREADS`: æ¯å€‹é€²ç¨‹çš„ç·šç¨‹æ•¸

**English**:
- `WORKERS`: Number of worker processes
- `THREADS`: Threads per worker process

---

## æ¨¡å‹è¨“ç·´æŒ‡å— | Model Training Guide

### æ•¸æ“šæº–å‚™ | Data Preparation

#### æ•¸æ“šæ ¼å¼ | Data Format

**ç¹é«”ä¸­æ–‡**ï¼š

è¨“ç·´æ•¸æ“šæ‡‰è©²æ˜¯å…©å€‹åŸŸçš„ç‰¹å¾µæ•¸æ“šï¼š

1. **Domain A (CPC ç‰¹å¾µ)**ï¼š
   - æ ¼å¼ï¼šTensor of shape `(N, 9)`
   - N: æ¨£æœ¬æ•¸é‡
   - 9: PC1-PC9 ç‰¹å¾µç¶­åº¦

2. **Domain B (MA ç‰¹å¾µ)**ï¼š
   - æ ¼å¼ï¼šTensor of shape `(M, 9)`
   - M: æ¨£æœ¬æ•¸é‡
   - 9: PC1-PC9 ç‰¹å¾µç¶­åº¦

**English**:

Training data should be feature data from two domains:

1. **Domain A (CPC Features)**: Tensor of shape `(N, 9)`
2. **Domain B (MA Features)**: Tensor of shape `(M, 9)`

#### æ•¸æ“šåŠ è¼‰ç¯„ä¾‹ | Data Loading Example

```python
import torch
from torch.utils.data import Dataset, DataLoader

class DualDomainDataset(Dataset):
    def __init__(self, domain_a_data, domain_b_data):
        """
        domain_a_data: numpy array of shape (N, 9)
        domain_b_data: numpy array of shape (M, 9)
        """
        self.domain_a = torch.FloatTensor(domain_a_data)
        self.domain_b = torch.FloatTensor(domain_b_data)

        # æ­£è¦åŒ–
        self.a_mean = torch.mean(self.domain_a, dim=0)
        self.a_std = torch.std(self.domain_a, dim=0)
        self.domain_a = (self.domain_a - self.a_mean) / (self.a_std + 1e-5)

        self.b_mean = torch.mean(self.domain_b, dim=0)
        self.b_std = torch.std(self.domain_b, dim=0)
        self.domain_b = (self.domain_b - self.b_mean) / (self.b_std + 1e-5)

        # æ·»åŠ ä½ç½®ç·¨ç¢¼
        pos_a = torch.linspace(0, 1, len(self.domain_a)).unsqueeze(1)
        pos_b = torch.linspace(0, 1, len(self.domain_b)).unsqueeze(1)

        self.domain_a = torch.cat([self.domain_a, pos_a], dim=1)
        self.domain_b = torch.cat([self.domain_b, pos_b], dim=1)

    def __len__(self):
        return min(len(self.domain_a), len(self.domain_b))

    def __getitem__(self, idx):
        return self.domain_a[idx], self.domain_b[idx % len(self.domain_b)]

# å‰µå»º DataLoader
dataset = DualDomainDataset(cpc_features, ma_features)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)
```

### è¨“ç·´åƒæ•¸è¨­ç½® | Training Parameters

#### è¶…åƒæ•¸é…ç½® | Hyperparameter Configuration

```python
# æ¨¡å‹åƒæ•¸ | Model Parameters
input_dim = 9          # PC ç‰¹å¾µç¶­åº¦
hidden_dim = 256       # æ½›åœ¨ç©ºé–“ç¶­åº¦

# è¨“ç·´åƒæ•¸ | Training Parameters
learning_rate = 0.0002  # Adam å­¸ç¿’ç‡
batch_size = 32         # æ‰¹æ¬¡å¤§å°
max_epochs = 100        # æœ€å¤§è¨“ç·´è¼ªæ•¸

# æå¤±æ¬Šé‡ | Loss Weights
mse_weight = 0.1        # MSE æå¤±æ¬Šé‡
cycle_weight = 0.5      # å¾ªç’°ä¸€è‡´æ€§æå¤±æ¬Šé‡
adversarial_weight = 0.4  # å°æŠ—æå¤±æ¬Šé‡
```

### å®Œæ•´è¨“ç·´è…³æœ¬ | Complete Training Script

```python
import pytorch_lightning as pl
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pl_module import PlMotorModule

# 1. æº–å‚™æ•¸æ“š
train_dataset = DualDomainDataset(train_cpc, train_ma)
val_dataset = DualDomainDataset(val_cpc, val_ma)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)

# 2. å‰µå»ºæ¨¡å‹
model = PlMotorModule(pl_set_input_dim=9)

# 3. è¨­ç½® Logger
logger = TensorBoardLogger("logs", name="cyclegan_training")

# 4. è¨­ç½® Callbacks
checkpoint_callback = ModelCheckpoint(
    dirpath="saves",
    filename="cyclegan-{epoch:02d}-{total_loss:.4f}",
    monitor="total_loss",
    mode="min",
    save_top_k=3
)

early_stopping = EarlyStopping(
    monitor="total_loss",
    patience=10,
    mode="min"
)

# 5. å‰µå»º Trainer
trainer = pl.Trainer(
    max_epochs=100,
    accelerator="gpu" if torch.cuda.is_available() else "cpu",
    devices=1,
    logger=logger,
    callbacks=[checkpoint_callback, early_stopping],
    log_every_n_steps=10,
    check_val_every_n_epoch=1
)

# 6. é–‹å§‹è¨“ç·´
trainer.fit(model, train_loader, val_loader)

# 7. ä¿å­˜æœ€çµ‚æ¨¡å‹
trainer.save_checkpoint("saves/final_model.ckpt")
print("Training completed! Model saved to saves/final_model.ckpt")
```

### ç›£æ§è¨“ç·´éç¨‹ | Monitoring Training

#### ä½¿ç”¨ TensorBoard

**ç¹é«”ä¸­æ–‡**ï¼š

```bash
# å•Ÿå‹• TensorBoard
tensorboard --logdir=logs

# åœ¨ç€è¦½å™¨ä¸­è¨ªå•
# http://localhost:6006
```

**ç›£æ§æŒ‡æ¨™**ï¼š
- `generator_loss/A_to_B`: Generator Aâ†’B æå¤±
- `generator_loss/B_to_A`: Generator Bâ†’A æå¤±
- `generator_loss/A_to_B_to_A`: å¾ªç’°æå¤± Aâ†’Bâ†’A
- `generator_loss/B_to_A_to_B`: å¾ªç’°æå¤± Bâ†’Aâ†’B
- `discriminator_loss/A`: Discriminator A æå¤±
- `discriminator_loss/B`: Discriminator B æå¤±
- `discriminator_acc/A`: Discriminator A æº–ç¢ºç‡
- `discriminator_acc/B`: Discriminator B æº–ç¢ºç‡

**English**:

```bash
# Start TensorBoard
tensorboard --logdir=logs

# Access in browser at http://localhost:6006
```

**Metrics to Monitor**:
- Generator losses for both directions
- Cycle consistency losses
- Discriminator losses and accuracies

### æ¨¡å‹è©•ä¼° | Model Evaluation

#### è©•ä¼°æŒ‡æ¨™ | Evaluation Metrics

```python
import torch
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

def evaluate_model(model, test_loader, device):
    model.eval()
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for cpc_feat, ma_feat in test_loader:
            cpc_feat = cpc_feat.to(device)
            ma_feat = ma_feat.to(device)

            # é æ¸¬
            ma_pred = model.generator_A_to_B(cpc_feat)

            all_predictions.append(ma_pred.cpu().numpy())
            all_targets.append(ma_feat[:, :-1].cpu().numpy())  # ç§»é™¤ä½ç½®ç·¨ç¢¼

    predictions = np.concatenate(all_predictions)
    targets = np.concatenate(all_targets)

    # è¨ˆç®—æŒ‡æ¨™
    mse = mean_squared_error(targets, predictions)
    mae = mean_absolute_error(targets, predictions)
    rmse = np.sqrt(mse)

    print(f"Evaluation Results:")
    print(f"  MSE: {mse:.6f}")
    print(f"  MAE: {mae:.6f}")
    print(f"  RMSE: {rmse:.6f}")

    return {"MSE": mse, "MAE": mae, "RMSE": rmse}

# ä½¿ç”¨ç¯„ä¾‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = PlMotorModule.load_from_checkpoint("saves/Batchnorm_version.ckpt")
model.to(device)

metrics = evaluate_model(model, test_loader, device)
```

### æ¨¡å‹å„ªåŒ–æŠ€å·§ | Model Optimization Tips

**ç¹é«”ä¸­æ–‡**ï¼š

1. **å­¸ç¿’ç‡èª¿æ•´**ï¼š
   - ä½¿ç”¨å­¸ç¿’ç‡èª¿åº¦å™¨ (LR Scheduler)
   - å»ºè­°ä½¿ç”¨ ReduceLROnPlateau æˆ– CosineAnnealingLR

2. **æ­£å‰‡åŒ–æŠ€è¡“**ï¼š
   - å¯ä»¥å•Ÿç”¨ Dropout (ç›®å‰è¨»è§£æ‰)
   - ä½¿ç”¨ Weight Decay

3. **æ•¸æ“šå¢å¼·**ï¼š
   - æ·»åŠ é«˜æ–¯å™ªè²
   - ç‰¹å¾µå°ºåº¦è®Šæ›

4. **æå¤±æ¬Šé‡èª¿æ•´**ï¼š
   - æ ¹æ“šé©—è­‰é›†è¡¨ç¾èª¿æ•´ä¸‰å€‹æå¤±çš„æ¬Šé‡
   - MSEã€Cycleã€Adversarial çš„å¹³è¡¡å¾ˆé‡è¦

**English**:

1. **Learning Rate Adjustment**: Use LR schedulers
2. **Regularization**: Enable Dropout, use Weight Decay
3. **Data Augmentation**: Add Gaussian noise, feature scaling
4. **Loss Weight Tuning**: Balance MSE, Cycle, and Adversarial losses

---

## API æ–‡æª” | API Documentation

### Health Check Endpoint

#### `GET /health`

**ç¹é«”ä¸­æ–‡**ï¼š

æª¢æŸ¥æœå‹™å¥åº·ç‹€æ…‹å’ŒåŸºæœ¬è³‡è¨Šã€‚

**è«‹æ±‚**ï¼š
```http
GET /health HTTP/1.1
Host: localhost:57122
```

**å›æ‡‰**ï¼š
```json
{
  "status": "healthy",
  "version": "1.0.0",
  "server_name": "customer_R_slice_Extract_Features",
  "instance_id": "a3f7b2c1",
  "queues": "analyze.step.sliced_wav_to_4col_AE_Features_324a"
}
```

**English**:

Check service health status and basic information.

**Request**:
```http
GET /health HTTP/1.1
Host: localhost:57122
```

**Response Fields**:
- `status`: Service status ("healthy" or "unhealthy")
- `version`: Service version
- `server_name`: Server identifier
- `instance_id`: Unique instance ID
- `queues`: RabbitMQ queue name being monitored

### ä½¿ç”¨ç¯„ä¾‹ | Usage Examples

#### Python Example

```python
import requests

def check_service_health(host="localhost", port=57122):
    """æª¢æŸ¥æœå‹™å¥åº·ç‹€æ…‹ | Check service health"""
    try:
        response = requests.get(f"http://{host}:{port}/health", timeout=5)

        if response.status_code == 200:
            data = response.json()
            print(f"âœ… Service is {data['status']}")
            print(f"   Version: {data['version']}")
            print(f"   Instance: {data['instance_id']}")
            return True
        else:
            print(f"âŒ Service returned status code: {response.status_code}")
            return False

    except requests.exceptions.RequestException as e:
        print(f"âŒ Failed to connect: {e}")
        return False

# ä½¿ç”¨ | Usage
check_service_health()
```

#### cURL Example

```bash
# åŸºæœ¬è«‹æ±‚ | Basic request
curl http://localhost:57122/health

# æ ¼å¼åŒ–è¼¸å‡º | Pretty print
curl -s http://localhost:57122/health | python -m json.tool

# æª¢æŸ¥ç‰¹å®šä¸»æ©Ÿ | Check specific host
curl http://192.168.1.100:57122/health

# åŒ…å«æ¨™é ­ | Include headers
curl -i http://localhost:57122/health

# è¨­ç½®è¶…æ™‚ | Set timeout
curl --max-time 5 http://localhost:57122/health
```

---

## ä½¿ç”¨ç¯„ä¾‹ | Usage Examples

### CPC åˆ° MA è½‰æ›å®Œæ•´ç¯„ä¾‹ | Complete CPC to MA Conversion Example

#### æº–å‚™è¼¸å…¥æ•¸æ“š | Prepare Input Data

**input.json**:
```json
[
  {
    "equID": 1.0,
    "faultID": "normal",
    "faultValue": 0.0,
    "PC1": -10.523,
    "PC2": 0.142,
    "PC3": 0.591,
    "PC4": -0.305,
    "PC5": 0.087,
    "PC6": 0.031,
    "PC7": 0.045,
    "PC8": 0.092,
    "PC9": -0.023
  },
  {
    "equID": 1.0,
    "faultID": "normal",
    "faultValue": 0.0,
    "PC1": -10.489,
    "PC2": 0.138,
    "PC3": 0.578,
    "PC4": -0.318,
    "PC5": 0.095,
    "PC6": 0.027,
    "PC7": 0.053,
    "PC8": 0.089,
    "PC9": -0.019
  }
]
```

#### åŸ·è¡Œè½‰æ› | Execute Conversion

```python
from cpc_to_ma_converter import CPCToMAConverter
import json

# æ–¹æ³• 1: ä½¿ç”¨é è¨­é…ç½® | Method 1: Use default config
converter = CPCToMAConverter()
converter.config["input_file"] = "input.json"
converter.config["output_file"] = "output.json"
converter.convert()

# æ–¹æ³• 2: ä½¿ç”¨è‡ªå®šç¾©é…ç½®æª”æ¡ˆ | Method 2: Use custom config
config = {
    "model_path": "saves/Batchnorm_version.ckpt",
    "input_file": "input.json",
    "output_file": "output.json",
    "ma_mean": [-10.458, 0.136, 0.583, -0.311, 0.093, 0.025, 0.051, 0.086, -0.018],
    "ma_std": [0.389, 0.991, 0.408, 0.478, 0.519, 0.381, 0.834, 0.710, 1.124],
    "preserve_metadata": True
}

with open("custom_config.json", "w") as f:
    json.dump(config, f, indent=4)

converter = CPCToMAConverter("custom_config.json")
converter.convert()
```

#### è¼¸å‡ºçµæœ | Output Result

**output.json**:
```json
[
  {
    "equID": 1.0,
    "faultID": "normal",
    "faultValue": 0.0,
    "PC1": -10.501,
    "PC2": 0.145,
    "PC3": 0.586,
    "PC4": -0.308,
    "PC5": 0.091,
    "PC6": 0.028,
    "PC7": 0.049,
    "PC8": 0.090,
    "PC9": -0.021
  }
]
```

### å®Œæ•´å·¥ä½œæµç¨‹ç¯„ä¾‹ | Complete Workflow Example

#### 1. ä¸Šå‚³éŸ³è¨Šæª”æ¡ˆä¸¦æäº¤ä»»å‹™ | Upload Audio and Submit Task

```python
import json
import pika
from pymongo import MongoClient
from gridfs import GridFS
from bson import ObjectId
import uuid

# é€£æ¥ MongoDB
client = MongoClient("mongodb://user:password@host:port")
db = client["sound_analysis"]
fs = GridFS(db)

# ç”Ÿæˆåˆ†æ UUID
analyze_uuid = str(uuid.uuid4())

# 1. ä¸Šå‚³éŸ³è¨Šæª”æ¡ˆåˆ° GridFS
with open("audio_sample.wav", "rb") as audio_file:
    file_id = fs.put(
        audio_file.read(),
        filename="audio_sample.wav",
        metadata={"AnalyzeUUID": analyze_uuid}
    )

print(f"âœ… Audio file uploaded. File ID: {file_id}")

# 2. å‰µå»ºåˆ†æä»»å‹™è¨˜éŒ„
analysis_doc = {
    "AnalyzeUUID": analyze_uuid,
    "AnalyzeState": "registered",
    "files": {
        "csv_transform": {
            "fileId": file_id,
            "filename": "audio_sample.wav",
            "type": "wav"
        }
    },
    "analyze_features": [
        {
            "features_step": 4,
            "features_state": "pending",
            "features_data": []
        }
    ]
}

db.analyses.insert_one(analysis_doc)
print(f"âœ… Analysis task created. UUID: {analyze_uuid}")

# 3. ç™¼é€ä»»å‹™æ¶ˆæ¯åˆ° RabbitMQ
connection = pika.BlockingConnection(
    pika.ConnectionParameters(
        host="rabbitmq-host",
        port=5672,
        credentials=pika.PlainCredentials("user", "password")
    )
)
channel = connection.channel()

message = {
    "AnalyzeUUID": analyze_uuid,
    "step": 4
}

channel.basic_publish(
    exchange="analyze.direct",
    routing_key="analyze.step.sliced_wav_to_4col_AE_Features_324a",
    body=json.dumps(message)
)

connection.close()
print(f"âœ… Task message sent to RabbitMQ")
```

#### 2. æŸ¥è©¢è™•ç†ç‹€æ…‹ | Query Processing Status

```python
import time

def check_task_status(analyze_uuid, db):
    """æŸ¥è©¢ä»»å‹™è™•ç†ç‹€æ…‹ | Check task processing status"""

    while True:
        analysis = db.analyses.find_one({"AnalyzeUUID": analyze_uuid})

        if not analysis:
            print("âŒ Task not found")
            return None

        step_4 = next((s for s in analysis["analyze_features"] if s["features_step"] == 4), None)

        if not step_4:
            print("âŒ Step 4 not found")
            return None

        state = step_4["features_state"]
        print(f"ğŸ“Š Task status: {state}")

        if state == "completed":
            print("âœ… Task completed successfully!")
            return step_4["features_data"]

        elif state == "error":
            error_msg = step_4.get("error_message", "Unknown error")
            print(f"âŒ Task failed: {error_msg}")
            return None

        elif state in ["pending", "processing"]:
            print(f"â³ Task is {state}, waiting...")
            time.sleep(5)
            continue

        else:
            print(f"âš ï¸ Unknown status: {state}")
            return None

# ä½¿ç”¨ | Usage
results = check_task_status(analyze_uuid, db)

if results:
    print(f"ğŸ“Š Received {len(results)} feature records")
    print(f"Sample result: {results[0]}")
```

#### 3. åŸ·è¡Œ CPC åˆ° MA è½‰æ› | Execute CPC to MA Conversion

```python
# å°‡çµæœè½‰æ›ç‚º CPC to MA converter å¯ä»¥ä½¿ç”¨çš„æ ¼å¼
import json

# ä¿å­˜ç‰¹å¾µæ•¸æ“š
with open("cpc_features.json", "w") as f:
    json.dump(results, f, indent=4)

# åŸ·è¡Œè½‰æ›
from cpc_to_ma_converter import CPCToMAConverter

converter = CPCToMAConverter()
converter.config["input_file"] = "cpc_features.json"
converter.config["output_file"] = "ma_features.json"
converter.convert()

# è®€å–çµæœ
with open("ma_features.json", "r") as f:
    ma_features = json.load(f)

print(f"âœ… Conversion completed!")
print(f"ğŸ“Š Generated {len(ma_features)} MA features")
```

### æ‰¹æ¬¡è™•ç†ç¯„ä¾‹ | Batch Processing Example

```python
import glob
import os
from concurrent.futures import ThreadPoolExecutor, as_completed

def process_single_file(input_file):
    """è™•ç†å–®å€‹æª”æ¡ˆ | Process single file"""
    try:
        output_file = input_file.replace("input", "output")

        converter = CPCToMAConverter()
        converter.config["input_file"] = input_file
        converter.config["output_file"] = output_file
        converter.convert()

        return {"status": "success", "file": input_file}

    except Exception as e:
        return {"status": "error", "file": input_file, "error": str(e)}

def batch_process_files(input_dir, max_workers=4):
    """æ‰¹æ¬¡è™•ç†å¤šå€‹æª”æ¡ˆ | Batch process multiple files"""

    # ç²å–æ‰€æœ‰è¼¸å…¥æª”æ¡ˆ
    input_files = glob.glob(os.path.join(input_dir, "*.json"))
    print(f"ğŸ“ Found {len(input_files)} files to process")

    results = []

    # ä½¿ç”¨ç·šç¨‹æ± è™•ç†
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_single_file, f): f for f in input_files}

        for future in as_completed(futures):
            result = future.result()
            results.append(result)

            if result["status"] == "success":
                print(f"âœ… Processed: {result['file']}")
            else:
                print(f"âŒ Failed: {result['file']} - {result['error']}")

    # çµ±è¨ˆçµæœ
    success_count = sum(1 for r in results if r["status"] == "success")
    error_count = len(results) - success_count

    print(f"\nğŸ“Š Batch processing completed:")
    print(f"   Success: {success_count}")
    print(f"   Failed: {error_count}")

    return results

# ä½¿ç”¨ | Usage
results = batch_process_files("./input_data", max_workers=4)
```

---

## é–‹ç™¼æŒ‡å— | Development Guide

### é–‹ç™¼ç’°å¢ƒé…ç½® | Development Environment Setup

#### 1. IDE è¨­ç½® | IDE Setup

**ç¹é«”ä¸­æ–‡**ï¼š

æ¨è–¦ä½¿ç”¨ PyCharm æˆ– VS Codeï¼š

**PyCharm**:
- å®‰è£ Python æ’ä»¶
- é…ç½® Python è§£é‡‹å™¨ç‚ºè™›æ“¬ç’°å¢ƒ
- å•Ÿç”¨ Type Hints æª¢æŸ¥

**VS Code**:
- å®‰è£ Python æ“´å±•
- å®‰è£ PyTorch æ“´å±•
- é…ç½® Pylint å’Œ Black æ ¼å¼åŒ–

**English**:

Recommended IDEs: PyCharm or VS Code

#### 2. ä»£ç¢¼é¢¨æ ¼ | Code Style

```python
# ä½¿ç”¨ Black æ ¼å¼åŒ–
pip install black
black .

# ä½¿ç”¨ Pylint æª¢æŸ¥
pip install pylint
pylint *.py

# ä½¿ç”¨ mypy é¡å‹æª¢æŸ¥
pip install mypy
mypy *.py
```

### æ·»åŠ æ–°åŠŸèƒ½ | Adding New Features

#### ç¯„ä¾‹ï¼šæ·»åŠ æ–°çš„ç”Ÿæˆå™¨æ¶æ§‹ | Example: Add New Generator Architecture

```python
# åœ¨ cycleGan_model.py ä¸­æ·»åŠ æ–°æ¨¡å‹
class MotorGeneratorConv(nn.Module):
    """åŸºæ–¼å·ç©çš„ç”Ÿæˆå™¨ | Convolution-based Generator"""

    def __init__(self, input_dim=9, hidden_dim=256):
        super().__init__()
        actual_input_dim = input_dim + 1

        # å°‡ 1D ç‰¹å¾µé‡å¡‘ç‚º 2D ä»¥ä½¿ç”¨å·ç©
        self.encoder = nn.Sequential(
            nn.Conv1d(1, 64, kernel_size=3, padding=1),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(64),
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(128),
        )

        self.decoder = nn.Sequential(
            nn.ConvTranspose1d(128, 64, kernel_size=3, padding=1),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(64),
            nn.ConvTranspose1d(64, 1, kernel_size=3, padding=1),
            nn.Tanh()
        )

    def forward(self, x):
        # x shape: (batch, 10)
        x = x.unsqueeze(1)  # (batch, 1, 10)
        x = self.encoder(x)
        x = self.decoder(x)
        x = x.squeeze(1)  # (batch, 10)
        return x[:, :-1]  # ç§»é™¤ä½ç½®ç·¨ç¢¼

# åœ¨ pl_module.py ä¸­ä½¿ç”¨æ–°æ¨¡å‹
from cycleGan_model import MotorGeneratorConv

class PlMotorModuleConv(pl.LightningModule):
    def __init__(self, pl_set_input_dim=9):
        super().__init__()
        self.automatic_optimization = False

        # ä½¿ç”¨æ–°çš„å·ç©ç”Ÿæˆå™¨
        self.generator_A_to_B = MotorGeneratorConv(input_dim=pl_set_input_dim)
        self.generator_B_to_A = MotorGeneratorConv(input_dim=pl_set_input_dim)

        # å…¶ä»–ä»£ç¢¼ä¿æŒä¸è®Š
        ...
```

### èª¿è©¦æŠ€å·§ | Debugging Tips

#### 1. å•Ÿç”¨è©³ç´°æ—¥èªŒ | Enable Verbose Logging

```python
import logging

# è¨­ç½®æ—¥èªŒç´šåˆ¥ç‚º DEBUG
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)
logger.debug("This is a debug message")
```

#### 2. ä½¿ç”¨ PyTorch çš„èª¿è©¦å·¥å…· | Use PyTorch Debug Tools

```python
import torch

# å•Ÿç”¨ anomaly detection
torch.autograd.set_detect_anomaly(True)

# æª¢æŸ¥æ¢¯åº¦
def check_gradients(model):
    for name, param in model.named_parameters():
        if param.grad is not None:
            print(f"{name}: grad_norm={param.grad.norm()}")
        else:
            print(f"{name}: no gradient")

# åœ¨è¨“ç·´å¾ªç’°ä¸­èª¿ç”¨
check_gradients(model)
```

#### 3. å¯è¦–åŒ–ç‰¹å¾µ | Visualize Features

```python
import matplotlib.pyplot as plt
import numpy as np

def visualize_features(cpc_features, ma_features, sample_idx=0):
    """å¯è¦–åŒ– CPC å’Œ MA ç‰¹å¾µ | Visualize CPC and MA features"""

    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # CPC ç‰¹å¾µ
    axes[0].bar(range(9), cpc_features[sample_idx])
    axes[0].set_title("CPC Features")
    axes[0].set_xlabel("Feature Index")
    axes[0].set_ylabel("Value")

    # MA ç‰¹å¾µ
    axes[1].bar(range(9), ma_features[sample_idx])
    axes[1].set_title("MA Features")
    axes[1].set_xlabel("Feature Index")
    axes[1].set_ylabel("Value")

    plt.tight_layout()
    plt.savefig("feature_comparison.png")
    plt.show()

# ä½¿ç”¨
visualize_features(cpc_data, ma_data, sample_idx=0)
```

---

## æ•…éšœæ’é™¤ | Troubleshooting

### å¸¸è¦‹å•é¡Œ | Common Issues

#### 1. R ç’°å¢ƒéŒ¯èª¤ | R Environment Error

**å•é¡Œ Problem**:
```
Error: R environment initialization failed
```

**è§£æ±ºæ–¹æ¡ˆ Solution**:

**ç¹é«”ä¸­æ–‡**:
1. ç¢ºèª R å·²æ­£ç¢ºå®‰è£
2. æª¢æŸ¥ rpy2 ç‰ˆæœ¬æ˜¯å¦å…¼å®¹
3. é©—è­‰ R å¥—ä»¶æ˜¯å¦å®Œæ•´å®‰è£

```bash
# æª¢æŸ¥ R ç‰ˆæœ¬
R --version

# é‡æ–°å®‰è£ R å¥—ä»¶
Rscript install_r_packages.R

# æ¸¬è©¦ rpy2
python -c "import rpy2.robjects as robjects; print('rpy2 OK')"
```

**English**:
1. Verify R is properly installed
2. Check rpy2 version compatibility
3. Verify R packages are fully installed

#### 2. MongoDB é€£æ¥å¤±æ•— | MongoDB Connection Failed

**å•é¡Œ Problem**:
```
pymongo.errors.ServerSelectionTimeoutError
```

**è§£æ±ºæ–¹æ¡ˆ Solution**:

```python
# æª¢æŸ¥é€£æ¥å­—ä¸²
from pymongo import MongoClient

try:
    client = MongoClient("mongodb://user:password@host:port", serverSelectionTimeoutMS=5000)
    client.server_info()
    print("âœ… MongoDB connection successful")
except Exception as e:
    print(f"âŒ MongoDB connection failed: {e}")
```

**ç¹é«”ä¸­æ–‡æª¢æŸ¥æ¸…å–®**:
- [ ] ç¢ºèª MongoDB æœå‹™æ­£åœ¨é‹è¡Œ
- [ ] æª¢æŸ¥é€£æ¥å­—ä¸²ä¸­çš„ç”¨æˆ¶åå’Œå¯†ç¢¼
- [ ] é©—è­‰ç¶²è·¯é€£æ¥å’Œé˜²ç«ç‰†è¨­ç½®
- [ ] æª¢æŸ¥ MongoDB ç‰ˆæœ¬å…¼å®¹æ€§

**English Checklist**:
- [ ] Verify MongoDB service is running
- [ ] Check username and password in connection string
- [ ] Verify network connectivity and firewall settings
- [ ] Check MongoDB version compatibility

#### 3. RabbitMQ æ¶ˆæ¯æœªè¢«æ¶ˆè²» | RabbitMQ Messages Not Consumed

**å•é¡Œ Problem**:
```
Messages are queued but not being processed
```

**è§£æ±ºæ–¹æ¡ˆ Solution**:

**ç¹é«”ä¸­æ–‡**:

1. æª¢æŸ¥ Consumer æ˜¯å¦æ­£åœ¨é‹è¡Œ
2. é©—è­‰ Routing Key æ˜¯å¦æ­£ç¢º
3. æª¢æŸ¥ Queue ç¶å®š

```bash
# ä½¿ç”¨ RabbitMQ ç®¡ç†ä»‹é¢
# http://rabbitmq-host:15672

# æˆ–ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·
rabbitmqctl list_queues
rabbitmqctl list_bindings
```

**English**:

1. Check if consumer is running
2. Verify routing key is correct
3. Check queue binding

#### 4. CUDA è¨˜æ†¶é«”ä¸è¶³ | CUDA Out of Memory

**å•é¡Œ Problem**:
```
RuntimeError: CUDA out of memory
```

**è§£æ±ºæ–¹æ¡ˆ Solution**:

```python
# æ–¹æ³• 1: æ¸›å°‘ batch size
batch_size = 16  # å¾ 32 æ¸›å°‘åˆ° 16

# æ–¹æ³• 2: æ¸…ç† GPU ç·©å­˜
import torch
torch.cuda.empty_cache()

# æ–¹æ³• 3: ä½¿ç”¨ CPU
device = torch.device("cpu")
model.to(device)

# æ–¹æ³• 4: å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é» (Gradient Checkpointing)
# åœ¨æ¨¡å‹ä¸­æ·»åŠ 
torch.utils.checkpoint.checkpoint(module, input)
```

#### 5. æ¨¡å‹è½‰æ›çµæœç•°å¸¸ | Model Conversion Results Abnormal

**å•é¡Œ Problem**:
```
Converted features have extreme values or NaN
```

**è§£æ±ºæ–¹æ¡ˆ Solution**:

```python
# æª¢æŸ¥æ•¸æ“šåˆ†ä½ˆ
import numpy as np

def check_data_quality(data, name="Data"):
    """æª¢æŸ¥æ•¸æ“šè³ªé‡ | Check data quality"""
    print(f"\n{name} Quality Check:")
    print(f"  Shape: {data.shape}")
    print(f"  Mean: {np.mean(data, axis=0)}")
    print(f"  Std: {np.std(data, axis=0)}")
    print(f"  Min: {np.min(data, axis=0)}")
    print(f"  Max: {np.max(data, axis=0)}")
    print(f"  NaN count: {np.isnan(data).sum()}")
    print(f"  Inf count: {np.isinf(data).sum()}")

check_data_quality(cpc_features, "CPC Features")
check_data_quality(ma_features, "MA Features")

# ä¿®æ­£ç•°å¸¸å€¼
def fix_abnormal_values(data, clip_range=(-100, 100)):
    """ä¿®æ­£ç•°å¸¸å€¼ | Fix abnormal values"""
    # æ›¿æ› NaN å’Œ Inf
    data = np.nan_to_num(data, nan=0.0, posinf=clip_range[1], neginf=clip_range[0])

    # è£å‰ªæ¥µç«¯å€¼
    data = np.clip(data, clip_range[0], clip_range[1])

    return data

ma_features = fix_abnormal_values(ma_features)
```

### æ—¥èªŒæŸ¥çœ‹ | Log Viewing

#### æ‡‰ç”¨ç¨‹å¼æ—¥èªŒ | Application Logs

```bash
# Docker å®¹å™¨æ—¥èªŒ
docker logs py_cyclegan

# æŒçºŒè·Ÿè¹¤æ—¥èªŒ
docker logs -f py_cyclegan

# æŸ¥çœ‹æœ€è¿‘ 100 è¡Œ
docker logs --tail 100 py_cyclegan

# æŸ¥çœ‹ç‰¹å®šæ™‚é–“ç¯„åœ
docker logs --since 2024-01-01T00:00:00 py_cyclegan
```

#### è¨“ç·´æ—¥èªŒ | Training Logs

```bash
# TensorBoard æ—¥èªŒ
tensorboard --logdir=logs --host=0.0.0.0 --port=6006

# æŸ¥çœ‹ Lightning æ—¥èªŒ
cat logs/cyclegan_training/version_0/metrics.csv
```

---

## ç¶­è­·èˆ‡å‡ç´š | Maintenance

### ç‰ˆæœ¬æ›´æ–°æµç¨‹ | Version Update Process

**ç¹é«”ä¸­æ–‡**ï¼š

1. **æ›´æ–°ä»£ç¢¼**ï¼š
   ```bash
   git pull origin master
   ```

2. **æ›´æ–° Dockerfile ç‰ˆæœ¬**ï¼š
   ```dockerfile
   ENV SERVER_VISION=1.0.7
   ```

3. **æ§‹å»ºæ–°æ˜ åƒ**ï¼š
   ```powershell
   .\scripts\build.ps1 1.0.7
   ```

4. **æ›´æ–°éƒ¨ç½²**ï¼š
   ```powershell
   .\scripts\deploy.ps1 1.0.7
   ```

5. **é©—è­‰æ›´æ–°**ï¼š
   ```bash
   curl http://localhost:57122/health
   ```

**English**:

1. **Update Code**: Pull latest changes
2. **Update Dockerfile Version**: Modify SERVER_VISION
3. **Build New Image**: Run build script
4. **Update Deployment**: Run deploy script
5. **Verify Update**: Check health endpoint

### æ•¸æ“šå‚™ä»½ | Data Backup

#### MongoDB å‚™ä»½ | MongoDB Backup

```bash
# å®Œæ•´å‚™ä»½
mongodump --uri="mongodb://user:password@host:port/sound_analysis" --out=/backup/$(date +%Y%m%d)

# åƒ…å‚™ä»½ç‰¹å®šé›†åˆ
mongodump --uri="mongodb://user:password@host:port/sound_analysis" --collection=analyses --out=/backup/analyses_$(date +%Y%m%d)

# æ¢å¾©å‚™ä»½
mongorestore --uri="mongodb://user:password@host:port/sound_analysis" /backup/20240101
```

#### æ¨¡å‹å‚™ä»½ | Model Backup

```bash
# å‚™ä»½æ¨¡å‹æª¢æŸ¥é»
tar -czf model_backup_$(date +%Y%m%d).tar.gz saves/

# ä¸Šå‚³åˆ°é›²ç«¯å­˜å„² (ç¯„ä¾‹)
aws s3 cp model_backup_20240101.tar.gz s3://my-bucket/backups/
```

### æ•ˆèƒ½å„ªåŒ– | Performance Optimization

#### 1. æ•¸æ“šåº«å„ªåŒ– | Database Optimization

```javascript
// MongoDB ç´¢å¼•å‰µå»º
db.analyses.createIndex({ "AnalyzeUUID": 1 })
db.analyses.createIndex({ "analyze_features.features_step": 1 })
db.analyses.createIndex({ "AnalyzeState": 1 })

// æª¢æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…æ³
db.analyses.explain("executionStats").find({"AnalyzeUUID": "some-uuid"})
```

#### 2. æ¨¡å‹æ¨ç†å„ªåŒ– | Model Inference Optimization

```python
# ä½¿ç”¨ TorchScript å„ªåŒ–
import torch

model = PlMotorModule.load_from_checkpoint("saves/Batchnorm_version.ckpt")
model.eval()

# è½‰æ›ç‚º TorchScript
scripted_model = torch.jit.script(model.generator_A_to_B)
torch.jit.save(scripted_model, "saves/generator_scripted.pt")

# è¼‰å…¥ä¸¦ä½¿ç”¨
scripted_model = torch.jit.load("saves/generator_scripted.pt")
output = scripted_model(input_tensor)
```

#### 3. RabbitMQ å„ªåŒ– | RabbitMQ Optimization

```python
# å¢åŠ  prefetch count ä»¥æé«˜ååé‡
channel.basic_qos(prefetch_count=10)

# ä½¿ç”¨æ‰¹æ¬¡ç¢ºèª
channel.basic_consume(queue=queue_name, on_message_callback=callback, auto_ack=False)

# åœ¨è™•ç†å¤šæ¢æ¶ˆæ¯å¾Œæ‰¹æ¬¡ç¢ºèª
if message_count % 10 == 0:
    channel.basic_ack(delivery_tag=method.delivery_tag, multiple=True)
```

---

## è²¢ç»æŒ‡å— | Contributing

### å¦‚ä½•è²¢ç» | How to Contribute

**ç¹é«”ä¸­æ–‡**ï¼š

1. Fork æœ¬å°ˆæ¡ˆ
2. å‰µå»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. é–‹å•Ÿ Pull Request

**English**:

1. Fork the project
2. Create feature branch
3. Commit changes
4. Push to branch
5. Open Pull Request

### ä»£ç¢¼è¦ç¯„ | Code Standards

- éµå¾ª PEP 8 é¢¨æ ¼æŒ‡å—
- æ·»åŠ é©ç•¶çš„è¨»è§£å’Œæ–‡æª”å­—ä¸²
- ç·¨å¯«å–®å…ƒæ¸¬è©¦
- ç¢ºä¿æ‰€æœ‰æ¸¬è©¦é€šé

**Follow PEP 8, add comments, write tests, ensure tests pass**

---

## æˆæ¬Š | License

æœ¬å°ˆæ¡ˆæ¡ç”¨ MIT æˆæ¬Šã€‚

This project is licensed under the MIT License.

---

## è¯çµ¡æ–¹å¼ | Contact

å¦‚æœ‰å•é¡Œæˆ–å»ºè­°ï¼Œè«‹é€šéä»¥ä¸‹æ–¹å¼è¯ç¹«ï¼š

For questions or suggestions, please contact:

- **Email**: [your-email@example.com](mailto:your-email@example.com)
- **Issue Tracker**: [GitHub Issues](https://github.com/your-repo/issues)

---

**æœ€å¾Œæ›´æ–° Last Updated**: 2025-10-27
**æ–‡æª”ç‰ˆæœ¬ Document Version**: 1.0.0
